{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Master Thesis Machine Learing  \n",
    "Gwen Hirsch  \n",
    "2022\n",
    "\n",
    "# Train ACD and Baselines on Power System and Weather Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### imports and variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\base2\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#own imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib import colors\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import pytz\n",
    "# from scipy import stats\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import gc\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_columns = 4000\n",
    "\n",
    "#acd imports\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lists that contain subsets of names later needed for filtering the dataframe columns\n",
    "\n",
    "# columns I want to predict\n",
    "vars_of_interest = ['load_actual', 'solar_generation', 'wind_generation' , 'price_day_ahead']\n",
    "\n",
    "# countries to include in dataset\n",
    "prefixes = ['DE_', 'FR_', 'CH_', 'GB_']\n",
    "\n",
    "# lists of the columns with variables of interest\n",
    "cols_of_interest = [prefix+var for prefix in prefixes for var in vars_of_interest]\n",
    "cols_of_interest_DE = ['DE_'+var for var in vars_of_interest]\n",
    "cols_of_interest_FR = ['FR_'+var for var in vars_of_interest]\n",
    "cols_of_interest_CH = ['CH_'+var for var in vars_of_interest]\n",
    "cols_of_interest_GB = ['GB_'+var for var in vars_of_interest]\n",
    "\n",
    "# lists of the columns with weather variables\n",
    "vars_weather = ['temperature', 'radiation_direct_horizontal', 'radiation_diffuse_horizontal']\n",
    "cols_weather = [prefix+var for prefix in prefixes for var in vars_weather]\n",
    "cols_weather_DE = ['DE_'+var for var in vars_weather]\n",
    "cols_weather_FR = ['FR_'+var for var in vars_weather]\n",
    "cols_weather_CH = ['CH_'+var for var in vars_weather]\n",
    "cols_weather_GB = ['GB_'+var for var in vars_weather]\n",
    "\n",
    "# all 7 quantities to predict\n",
    "vars_predict = vars_of_interest+vars_weather\n",
    "\n",
    "# save time column names\n",
    "cols_time = ['year', 'month', 'day', 'hour', 'weekday']\n",
    "\n",
    "# save column names in lists and with different orders for plotting...\n",
    "# ...according to countries\n",
    "all_vars_of_interest = [prefix+var for prefix in prefixes for var in vars_of_interest+vars_weather]\n",
    "all_vars_of_interest_DE = cols_of_interest_DE + cols_weather_DE\n",
    "all_vars_of_interest_FR = cols_of_interest_FR + cols_weather_FR\n",
    "all_vars_of_interest_CH = cols_of_interest_CH + cols_weather_CH\n",
    "all_vars_of_interest_GB = cols_of_interest_GB + cols_weather_GB\n",
    "\n",
    "# ...according to variables\n",
    "all_vars_of_interest_varfirst = [prefix+var for var in vars_of_interest+vars_weather for prefix in prefixes]\n",
    "\n",
    "# all original columns in final dataset\n",
    "all_cols_to_keep     = ['utc_timestamp']+ all_vars_of_interest +cols_time\n",
    "all_cols_to_keep_DE  = ['utc_timestamp']+ [var for var in all_vars_of_interest if var.startswith('DE_')] +cols_time\n",
    "all_cols_to_keep_FR  = ['utc_timestamp']+ [var for var in all_vars_of_interest if var.startswith('FR_')] +cols_time\n",
    "all_cols_to_keep_CH  = ['utc_timestamp']+ [var for var in all_vars_of_interest if var.startswith('CH_')] +cols_time\n",
    "all_cols_to_keep_GB  = ['utc_timestamp']+ [var for var in all_vars_of_interest if var.startswith('GB_')] +cols_time\n",
    "\n",
    "# used in dataset generation\n",
    "# contains all aditional time features \n",
    "cols_ts_DE = all_vars_of_interest_DE\n",
    "cols_ts_FR = all_vars_of_interest_FR\n",
    "cols_ts_CH = all_vars_of_interest_CH\n",
    "cols_ts_GB = all_vars_of_interest_GB\n",
    "cols_ts_all = all_vars_of_interest\n",
    "cols_ts_long = vars_of_interest+vars_weather\n",
    "cols_to_cycle = ['month', 'day', 'hour', 'weekday']\n",
    "cols_timedims_cycle = [timedim+version for timedim in cols_to_cycle for version in ['_sin', '_cos']]\n",
    "cols_to_normalize_DE = cols_ts_DE+['year']\n",
    "cols_to_normalize_all = cols_ts_all+['year']\n",
    "cols_to_normalize_long = cols_ts_long+['year', 'ID']\n",
    "\n",
    "idx_firstmonday = 96\n",
    "idx_lastsunday = 43775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labels for correlation matrix\n",
    "\n",
    "# labels if correlation matrix is done for Germany only\n",
    "labels_vars_full = {}\n",
    "for v in all_vars_of_interest:\n",
    "    if 'load_actual' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' total load', '[MW]')\n",
    "    elif 'load_forecast' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' load forecast', '[MW]')\n",
    "    elif 'solar_generation' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' solar generation', '[MW]')\n",
    "    elif 'wind_generation' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' wind generation', '[MW]')\n",
    "    elif 'price_day_ahead' in v:\n",
    "        if v.partition('_')[0] != 'GB':\n",
    "            labels_vars_full[v] = (v.partition('_')[0]+' price day-ahead', '[EUR]')\n",
    "        else:\n",
    "            labels_vars_full[v] = (v.partition('_')[0]+' price day-ahead', '[GBP]')\n",
    "    elif 'temperature' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' temperature', '[°C]')\n",
    "    elif 'radiation_direct_horizontal' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' radiation direct', '[W/m²]')\n",
    "    elif 'radiation_diffuse_horizontal' in v:\n",
    "        labels_vars_full[v] = (v.partition('_')[0]+' radiation diffuse', '[W/m²]')\n",
    "        \n",
    "# labels if correlation matrix is done for all countries\n",
    "labels_vars_short = {}\n",
    "for v in all_vars_of_interest:\n",
    "    if 'load_actual' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' LO', '[MW]')\n",
    "    elif 'solar_generation' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' SO', '[MW]')\n",
    "    elif 'wind_generation' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' WI', '[MW]')\n",
    "    elif 'price_day_ahead' in v:\n",
    "        if v.partition('_')[0] != 'GB':\n",
    "            labels_vars_short[v] = (v.partition('_')[0]+' PR', '[EUR]')\n",
    "        else:\n",
    "            labels_vars_short[v] = (v.partition('_')[0]+' PR', '[GBP]')\n",
    "    elif 'temperature' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' TE', '[°C]')\n",
    "    elif 'radiation_direct_horizontal' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' R1', '[W/m²]')\n",
    "    elif 'radiation_diffuse_horizontal' in v:\n",
    "        labels_vars_short[v] = (v.partition('_')[0]+' R2', '[W/m²]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### settings for training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose dataset\n",
    "suffix = \"_energy_DE_1d\"\n",
    "suffix = \"_energy_all_1d\"\n",
    "suffix = \"_energy_long_1d\"\n",
    "# suffix = \"_energy_DE_7d\"\n",
    "# suffix = \"_energy_all_7d\"\n",
    "# suffix = \"_energy_long_7d\"\n",
    "\n",
    "# choose acd version -> either a gcn encoder or variational distribution\n",
    "version = 'gcn'\n",
    "# version = 'variational'\n",
    "\n",
    "# choose lstm_format=True if data set has to have lstm format (e.g. for rnn baseline) \n",
    "# NOTE: run_rnn has to be set to True for the rnn baseline to train\n",
    "lstm_format = False \n",
    "# lstm_format = True\n",
    "\n",
    "# whether to run ACD model\n",
    "run_acd = False\n",
    "# run_acd = True\n",
    "if run_acd:\n",
    "    if lstm_format:\n",
    "        lstm_format = False\n",
    "\n",
    "# whether to run MLR baseline\n",
    "run_mlr = False\n",
    "run_mlr = True\n",
    "if run_mlr:\n",
    "    if lstm_format:\n",
    "        lstm_format = False\n",
    "\n",
    "# whether to run LSTM baseline\n",
    "run_rnn = False\n",
    "# run_rnn = True\n",
    "if run_rnn:\n",
    "    if not lstm_format:\n",
    "        lstm_format = True\n",
    "\n",
    "# further settings\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "no_cuda = False\n",
    "# no_cuda=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Specify settings and load normed dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    ".npy input files for acd have shape  \n",
    "[num_samples, num_ts, num_timesteps, num_dims]   \n",
    "e.g. [3*365, 7, 24/48, 1+num_features]  \n",
    "(in acd code [num_sims, num_atoms, num_timesteps, num_dims]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### load dataframe and norm params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if suffix == \"_energy_DE_1d\":\n",
    "    num_samples_train = 3*365+1 \n",
    "    num_samples_valid = 365\n",
    "    num_samples_test = 365\n",
    "    cols_ts = cols_ts_DE\n",
    "    cols_timedims = ['year']+cols_timedims_cycle\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24\n",
    "    num_dims = 1 +len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format = True\n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "    \n",
    "if suffix == \"_energy_all_1d\":\n",
    "    num_samples_train = 3*365+1 \n",
    "    num_samples_valid = 365\n",
    "    num_samples_test = 365\n",
    "    cols_ts = cols_ts_all\n",
    "    cols_timedims = ['year']+cols_timedims_cycle\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24\n",
    "    num_dims = 1 +len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format = True\n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "        \n",
    "if suffix == \"_energy_long_1d\":\n",
    "    num_samples_train = (3*365+1)*len(prefixes)\n",
    "    num_samples_valid = 365*len(prefixes)\n",
    "    num_samples_test = 365*len(prefixes)\n",
    "    cols_ts = cols_ts_long\n",
    "    cols_timedims = ['year']+cols_timedims_cycle+['ID']\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24\n",
    "    num_dims = 1 +len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format=True \n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "        \n",
    "if suffix == \"_energy_DE_7d\":\n",
    "    num_samples_train = 3*52\n",
    "    num_samples_valid = 52\n",
    "    num_samples_test = 52\n",
    "    cols_ts = cols_ts_DE\n",
    "    cols_timedims = ['year']+cols_timedims_cycle\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24*7\n",
    "    num_dims = 1 +len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format=True \n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    idx_start = idx_firstmonday#df['weekday'][df['weekday'] == 0].index[0]\n",
    "    idx_end = idx_firstmonday+260*7*24    \n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "        \n",
    "if suffix == \"_energy_all_7d\":\n",
    "    num_samples_train = 3*52\n",
    "    num_samples_valid = 52\n",
    "    num_samples_test  = 52\n",
    "    cols_ts = cols_ts_all\n",
    "    cols_timedims = ['year']+cols_timedims_cycle\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24*7\n",
    "    num_dims = 1 + len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format=True \n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    idx_start = idx_firstmonday#df['weekday'][df['weekday'] == 0].index[0]\n",
    "    idx_end = idx_firstmonday+260*7*24    \n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "        \n",
    "if suffix == \"_energy_long_7d\":\n",
    "    num_samples_train = 3*52*len(prefixes)\n",
    "    num_samples_valid = 52*len(prefixes)\n",
    "    num_samples_test = 52*len(prefixes)\n",
    "    cols_ts = cols_ts_long\n",
    "    cols_timedims = ['year']+cols_timedims_cycle+['ID']\n",
    "    num_ts = len(cols_ts)\n",
    "    num_timesteps = 24*7\n",
    "    num_dims = 1 +len(cols_timedims)\n",
    "    if run_rnn or lstm_format:\n",
    "        lstm_format=True \n",
    "        suffix = suffix[:-3]+'_lstm'\n",
    "        num_samples_train = num_samples_train*num_timesteps\n",
    "        num_samples_valid = num_samples_valid*num_timesteps\n",
    "        num_samples_test = num_samples_test*num_timesteps\n",
    "        num_dims = num_ts + len(cols_timedims)\n",
    "        num_timesteps=1\n",
    "    idx_start = idx_firstmonday*len(prefixes)\n",
    "    idx_end = idx_firstmonday*len(prefixes)+260*7*24*len(prefixes)\n",
    "    dataframe = pd.read_csv('data/normed' +suffix+'.csv', parse_dates=['utc_timestamp'])\n",
    "    df_norm_params = pd.read_csv('data/norm_params' +suffix+'.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case some shape error comes up, comment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feat_train_energyx = np.load('data/feat_train' +suffix+'.npy')\n",
    "# feat_valid_energyx = np.load('data/feat_valid' +suffix+'.npy')\n",
    "# feat_test_energyx = np.load('data/feat_test' +suffix+'.npy')\n",
    "# feat_predict_energyx = np.load('data/feat_predict' +suffix+'.npy')\n",
    "\n",
    "# edges_train_energyx    = np.load('data/edges_train' +suffix+'.npy')\n",
    "# edges_valid_energyx    = np.load('data/edges_valid' +suffix+'.npy')\n",
    "# edges_test_energyx     = np.load('data/edges_test' +suffix+'.npy')\n",
    "# edges_predict_energyx  = np.load('data/edges_predict' +suffix+'.npy')\n",
    "# norm_params_energyx    = pd.read_csv('data/norm_params' +suffix+ '.csv')\n",
    "\n",
    "# print(feat_train_energyx.shape)\n",
    "# print(feat_valid_energyx.shape)\n",
    "# print(feat_test_energyx.shape)\n",
    "# print(feat_predict_energyx.shape)\n",
    "# if run_rnn:\n",
    "#     target_train_energyx = np.load('data/target_train' +suffix+'.npy')\n",
    "#     target_valid_energyx = np.load('data/target_valid' +suffix+'.npy')\n",
    "#     target_test_energyx  = np.load('data/target_test' +suffix+'.npy')\n",
    "#     print(target_train_energyx.shape)\n",
    "#     print(target_valid_energyx.shape)\n",
    "#     print(target_test_energyx.shape)\n",
    "# print(edges_train_energyx.shape)\n",
    "# print(edges_valid_energyx.shape)\n",
    "# print(edges_test_energyx.shape)\n",
    "# print(edges_predict_energyx.shape)\n",
    "# display(norm_params_energyx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- ACD Code --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This acd implementation is adapted from the original implementation at  \n",
    "https://github.com/loeweX/AmortizedCausalDiscovery.  \n",
    "All individual files from there are stored in ACD_code.py\n",
    "\n",
    "Optionally, instead of importing the .py file, one can copy the cells from the    \n",
    "ACD_code_notebook.ipynb in here for easier handling when editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ACD_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# from model.modules import *\n",
    "# from utils import arg_parser, logger, data_loader, forward_pass_and_eval\n",
    "# from model import utils, model_loader\n",
    "\n",
    "\n",
    "def train():\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        t_epoch = time.time()\n",
    "        train_losses = defaultdict(list)\n",
    "\n",
    "        for batch_idx, minibatch in enumerate(train_loader):\n",
    "\n",
    "            data, relations, temperatures = unpack_batches(args, minibatch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            losses, _, _, _ = forward_pass_and_eval(\n",
    "                args,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                data,\n",
    "                relations,\n",
    "                rel_rec,\n",
    "                rel_send,\n",
    "                args.hard,\n",
    "                edge_probs=edge_probs,\n",
    "                log_prior=log_prior,\n",
    "                temperatures=temperatures,\n",
    "            )\n",
    "\n",
    "            loss = losses[\"loss\"]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses = append_losses(train_losses, losses)\n",
    "\n",
    "        string = logs.result_string(\"train\", epoch, train_losses, t=t_epoch)\n",
    "        logs.write_to_log_file(string)\n",
    "        logs.append_train_loss(train_losses)\n",
    "        scheduler.step()\n",
    "\n",
    "        if args.validate:\n",
    "            val_losses = val(epoch)\n",
    "            val_loss = np.mean(val_losses[\"loss\"])\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\"Best model so far, saving...\")\n",
    "                logs.create_log(\n",
    "                    args,\n",
    "                    encoder=encoder,\n",
    "                    decoder=decoder,\n",
    "                    optimizer=optimizer,\n",
    "                    accuracy=np.mean(val_losses[\"acc\"]),\n",
    "                )\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        elif (epoch + 1) % 100 == 0:\n",
    "            logs.create_log(\n",
    "                args,\n",
    "                encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                optimizer=optimizer,\n",
    "                accuracy=np.mean(train_losses[\"acc\"]),\n",
    "            )\n",
    "\n",
    "        logs.draw_loss_curves()\n",
    "\n",
    "    return best_epoch, epoch\n",
    "\n",
    "\n",
    "def val(epoch):\n",
    "    t_val = time.time()\n",
    "    val_losses = defaultdict(list)\n",
    "\n",
    "    if args.use_encoder:\n",
    "        encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    for batch_idx, minibatch in enumerate(valid_loader):\n",
    "\n",
    "        data, relations, temperatures = unpack_batches(args, minibatch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses, _, _, _ = forward_pass_and_eval(\n",
    "                args,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                data,\n",
    "                relations,\n",
    "                rel_rec,\n",
    "                rel_send,\n",
    "                True,\n",
    "                edge_probs=edge_probs,\n",
    "                log_prior=log_prior,\n",
    "                testing=True,\n",
    "                temperatures=temperatures,\n",
    "            )\n",
    "\n",
    "        val_losses = append_losses(val_losses, losses)\n",
    "\n",
    "    string = logs.result_string(\"validate\", epoch, val_losses, t=t_val)\n",
    "    logs.write_to_log_file(string)\n",
    "    logs.append_val_loss(val_losses)\n",
    "\n",
    "    if args.use_encoder:\n",
    "        encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    return val_losses\n",
    "\n",
    "\n",
    "def test(encoder, decoder, epoch):\n",
    "    args.shuffle_unobserved = False\n",
    "    # args.prediction_steps = 49\n",
    "    test_losses = defaultdict(list)\n",
    "\n",
    "    if args.load_folder == \"\":\n",
    "        ## load model that had the best validation performance during training\n",
    "        if args.use_encoder:\n",
    "            encoder.load_state_dict(torch.load(args.encoder_file))\n",
    "        decoder.load_state_dict(torch.load(args.decoder_file))\n",
    "\n",
    "    if args.use_encoder:\n",
    "        encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    for batch_idx, minibatch in enumerate(test_loader):\n",
    "\n",
    "        data, relations, temperatures = unpack_batches(args, minibatch)\n",
    "        # print(data.shape)\n",
    "        # print(relations.shape)\n",
    "        # print(data.size(2))\n",
    "        # print(args.timesteps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            assert (data.size(2) - args.timesteps) >= args.timesteps\n",
    "\n",
    "            data_encoder = data[:, :, : args.timesteps, :].contiguous()\n",
    "            data_decoder = data[:, :, args.timesteps : -1, :].contiguous()\n",
    "\n",
    "            losses, _, _, _, = forward_pass_and_eval(\n",
    "                args,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                data,\n",
    "                relations,\n",
    "                rel_rec,\n",
    "                rel_send,\n",
    "                True,\n",
    "                data_encoder=data_encoder,\n",
    "                data_decoder=data_decoder,\n",
    "                edge_probs=edge_probs,\n",
    "                log_prior=log_prior,\n",
    "                testing=True,\n",
    "                temperatures=temperatures,\n",
    "            )\n",
    "\n",
    "        test_losses = append_losses(test_losses, losses)\n",
    "\n",
    "    string = logs.result_string(\"test\", epoch, test_losses)\n",
    "    logs.write_to_log_file(string)\n",
    "    logs.append_test_loss(test_losses)\n",
    "\n",
    "    logs.create_log(\n",
    "        args,\n",
    "        decoder=decoder,\n",
    "        encoder=encoder,\n",
    "        optimizer=optimizer,\n",
    "        final_test=True,\n",
    "        test_losses=test_losses,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- MLR Code --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_MLR(df, cols_ts, cols_timedims, \n",
    "            num_samples_train=(3*365+1)*24,\n",
    "            num_samples_valid=365*24, \n",
    "            num_samples_test=365*24,\n",
    "            epochs=100,\n",
    "            suffix='_energy',\n",
    "            print_every=100,\n",
    "            lr=0.1\n",
    "            ):\n",
    "    \n",
    "    num_ts = len(cols_ts)\n",
    "    len_df = len(df)\n",
    "    offset_valid = num_samples_train\n",
    "    offset_test = num_samples_train+num_samples_valid\n",
    "    linear_regression_models = {}\n",
    "    test_losses = {}\n",
    "    for target in cols_ts:\n",
    "        x_whole = torch.FloatTensor(df.loc[:(len_df-1)-1, cols_ts+cols_timedims].values).T\n",
    "        y_whole = torch.FloatTensor(df.loc[1:(len_df-1), target].values).T\n",
    "        \n",
    "        x_train = torch.FloatTensor(df.loc[0:num_samples_train-1, cols_ts+cols_timedims].values.T)#.values)\n",
    "        y_train = torch.FloatTensor(df.loc[1:num_samples_train, target].values.T)#.values)\n",
    "        \n",
    "        x_valid = torch.FloatTensor(df.loc[offset_valid+0:offset_valid+num_samples_valid-1, \n",
    "                                          cols_ts+cols_timedims].values).T\n",
    "        y_valid = torch.FloatTensor(df.loc[offset_valid+1:offset_valid+num_samples_valid, \n",
    "                                          target].values).T        \n",
    "        \n",
    "        x_test = torch.FloatTensor(df.loc[offset_test+0:offset_test+num_samples_test-1-1, \n",
    "                                          cols_ts+cols_timedims].values).T\n",
    "        y_test = torch.FloatTensor(df.loc[offset_test+1:offset_test+num_samples_test-1, \n",
    "                                          target].values).T\n",
    "        \n",
    "        \n",
    "        n_predictors = x_train.shape[0]\n",
    "        A = torch.randn((1, n_predictors), requires_grad=True)\n",
    "        b = torch.randn(1, requires_grad=True)\n",
    "        \n",
    "        optimizer = optim.Adam([A, b], lr=lr)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_pred = A.mm(x_train) + b\n",
    "            train_loss = ((train_pred - y_train)**2).sum()/num_samples_train\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            valid_pred = A.detach()@x_valid + b.detach()\n",
    "            val_loss = ((valid_pred - y_valid)**2).sum()/num_samples_valid\n",
    "            \n",
    "            if (i+1)%print_every==0:\n",
    "                string = f\"i={i+1} target {target} train_loss {train_loss} val_loss {val_loss}\" \n",
    "                logs.write_to_log_file(string)\n",
    "                \n",
    "                \n",
    "        test_pred = A.detach()@x_test + b.detach()\n",
    "        test_loss = ((test_pred - y_test)**2).sum()/num_samples_test\n",
    "        test_losses[target]=test_loss.item()\n",
    "        \n",
    "        string = f\"target = {target}, test loss = {test_loss}\" \n",
    "        logs.write_to_log_file(string)   \n",
    "        \n",
    "        linear_regression_models[target]=(A, b)\n",
    "        \n",
    "        df[target+'_pred'] = 0\n",
    "        df.loc[1:,target+'_pred'] = (A.detach()@x_whole + b.detach())[0].tolist()\n",
    "        \n",
    "    avg_test_loss = np.mean(list(test_losses.values()))\n",
    "    string = f\"average test loss= {avg_test_loss}\" \n",
    "    logs.write_to_log_file(string)   \n",
    "    \n",
    "    return linear_regression_models, df, test_losses, avg_test_loss\n",
    "\n",
    "def predict_MLR(df, MLR_models, cols_ts, cols_timedims):\n",
    "    for target in cols_ts:\n",
    "        x = torch.FloatTensor(df.loc[0:num_samples_train-1,\n",
    "                                           cols_ts+cols_timedims].values).T\n",
    "        y = torch.FloatTensor(df.loc[1:num_samples_train, \n",
    "                                           target].values).T\n",
    "        (A, b) = MLR_models[target]\n",
    "\n",
    "        df[target+'_pred'] = 0\n",
    "        df.loc[1:,target+'_pred'] = (A.detach()@x + b.detach())[0].tolist()        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- RNN Code --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN_baseline(nn.Module):\n",
    "    def __init__(self, lstm_layers, input_dim, target_dim, batch_size, hidden_dim, device):\n",
    "        super(RNN_baseline, self).__init__()\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.target_dim = target_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.lstm_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.target_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        hidden_state = torch.zeros(self.lstm_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        cell_state = torch.zeros(self.lstm_layers, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        \n",
    "        outputs, hidden = self.lstm(inputs)\n",
    "        \n",
    "        return self.linear(outputs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_rnn(args):\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        t_epoch = time.time()\n",
    "        train_losses = defaultdict(list)\n",
    "        \n",
    "        (train_loader, \n",
    "         valid_loader,\n",
    "         test_loader,\n",
    "         loc_max,\n",
    "         loc_min,\n",
    "         vel_max,\n",
    "         vel_min,\n",
    "        ) = load_data(args)\n",
    "\n",
    "        for batch_idx, minibatch in enumerate(train_loader):\n",
    "            \n",
    "            data, target, _ = unpack_batches(args, minibatch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses, output = forward_pass_rnn(\n",
    "                args,\n",
    "                rnn,\n",
    "                data,\n",
    "                target)\n",
    "\n",
    "            loss = losses[\"loss_mse\"]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_losses = append_losses(train_losses, losses)\n",
    "\n",
    "        string = logs.result_string(\"train\", epoch, train_losses, t=t_epoch)\n",
    "        logs.write_to_log_file(string)\n",
    "        logs.append_train_loss(train_losses)\n",
    "        scheduler.step()\n",
    "\n",
    "        if args.validate:\n",
    "            val_losses = val_rnn(epoch, rnn_args)\n",
    "            val_loss = np.mean(val_losses[\"loss_mse\"])\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\"Best model so far, saving...\")\n",
    "                logs.create_log(\n",
    "                    rnn_args,\n",
    "                    rnn=rnn,\n",
    "                    optimizer=optimizer,\n",
    "                )\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        elif (epoch + 1) % 100 == 0:\n",
    "            logs.create_log(\n",
    "                rnn_args,\n",
    "                rnn=rnn,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "\n",
    "        logs.draw_loss_curves()\n",
    "\n",
    "    return best_epoch, epoch\n",
    "\n",
    "\n",
    "def val_rnn(epoch, args):\n",
    "    t_val = time.time()\n",
    "    val_losses = defaultdict(list)\n",
    "\n",
    "    rnn.eval()\n",
    "\n",
    "    for batch_idx, minibatch in enumerate(valid_loader):\n",
    "\n",
    "        data, target, _ = unpack_batches(args, minibatch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses, output = forward_pass_rnn(\n",
    "                args,\n",
    "                rnn,\n",
    "                data,\n",
    "                target\n",
    "            )\n",
    "\n",
    "        val_losses = append_losses(val_losses, losses)\n",
    "\n",
    "    string = logs.result_string(\"validate\", epoch, val_losses, t=t_val)\n",
    "    logs.write_to_log_file(string)\n",
    "    logs.append_val_loss(val_losses)\n",
    "\n",
    "    rnn.train()\n",
    "\n",
    "    return val_losses\n",
    "\n",
    "\n",
    "def test_rnn(rnn, epoch, args):\n",
    "    test_losses = defaultdict(list)\n",
    "\n",
    "    if args.load_folder == \"\":\n",
    "        ## load model that had the best validation performance during training\n",
    "        rnn.load_state_dict(torch.load(args.rnn_file))\n",
    "    \n",
    "    rnn.eval()    \n",
    "    \n",
    "    for batch_idx, minibatch in enumerate(test_loader):\n",
    "\n",
    "        data, target, _ = unpack_batches(args, minibatch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses, output = forward_pass_rnn(\n",
    "                args,\n",
    "                rnn,\n",
    "                data,\n",
    "                target#.unsqueeze(1)\n",
    "            )\n",
    "\n",
    "        test_losses = append_losses(test_losses, losses)\n",
    "\n",
    "    string = logs.result_string(\"test\", epoch, test_losses)\n",
    "    logs.write_to_log_file(string)\n",
    "    logs.append_test_loss(test_losses)\n",
    "\n",
    "    logs.create_log(   \n",
    "        args,\n",
    "        rnn=rnn,\n",
    "        optimizer=optimizer,\n",
    "        final_test=True,\n",
    "        test_losses=test_losses\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- run ACD on energy data --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_samples=num_samples_train\n",
    "test_samples=num_samples_test\n",
    "timesteps=num_timesteps\n",
    "num_atoms=num_ts\n",
    "dims=num_dims\n",
    "if version == 'gcn':\n",
    "    dont_use_encoder = False\n",
    "elif version == 'variational':\n",
    "    dont_use_encoder = True \n",
    "\n",
    "args = parse_args(\n",
    "    epochs=epochs, \n",
    "    training_samples=training_samples, \n",
    "    test_samples=test_samples,\n",
    "    shuffle_traindata=True,\n",
    "    suffix=suffix,\n",
    "    timesteps=timesteps,\n",
    "    num_atoms=num_atoms,\n",
    "    dims=dims,\n",
    "    no_cuda=no_cuda,\n",
    "    dont_use_encoder=dont_use_encoder,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_acd:\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting_time: {datetime.datetime.fromtimestamp(start_time)}\")\n",
    "    \n",
    "    logs = Logger(args)\n",
    "\n",
    "    if args.GPU_to_use is not None:\n",
    "        logs.write_to_log_file(\"Using GPU #\" + str(args.GPU_to_use))\n",
    "\n",
    "    (\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        test_loader,\n",
    "        loc_max,\n",
    "        loc_min,\n",
    "        vel_max,\n",
    "        vel_min,\n",
    "    ) = load_data(args)\n",
    "\n",
    "    rel_rec, rel_send = create_rel_rec_send(args, args.num_atoms)\n",
    "\n",
    "    encoder, decoder, optimizer, scheduler, edge_probs = load_model(\n",
    "        args, loc_max, loc_min, vel_max, vel_min\n",
    "    )\n",
    "\n",
    "    logs.write_to_log_file(encoder)\n",
    "    logs.write_to_log_file(decoder)\n",
    "\n",
    "    if args.prior != 1:\n",
    "        assert 0 <= args.prior <= 1, \"args.prior not in the right range\"\n",
    "        prior = np.array(\n",
    "            [args.prior]\n",
    "            + [\n",
    "                (1 - args.prior) / (args.edge_types - 1)\n",
    "                for _ in range(args.edge_types - 1)\n",
    "            ]\n",
    "        )\n",
    "        logs.write_to_log_file(\"Using prior\")\n",
    "        logs.write_to_log_file(prior)\n",
    "        log_prior = torch.FloatTensor(np.log(prior))\n",
    "        log_prior = log_prior.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        if args.cuda:\n",
    "            log_prior = log_prior.cuda()\n",
    "    else:\n",
    "        log_prior = None\n",
    "\n",
    "    if args.global_temp:\n",
    "        args.categorical_temperature_prior = get_categorical_temperature_prior(\n",
    "            args.alpha, args.num_cats, to_cuda=args.cuda\n",
    "        )\n",
    "\n",
    "    ##Train model\n",
    "    try:\n",
    "        if args.test_time_adapt:\n",
    "            raise KeyboardInterrupt\n",
    "\n",
    "        best_epoch, epoch = train()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        best_epoch, epoch = -1, -1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    logs.write_to_log_file(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "\n",
    "    if args.test:\n",
    "        test(encoder, decoder, epoch)\n",
    "    \n",
    "    if args.dont_use_encoder:\n",
    "        edge_probs_path = os.path.join(args.log_path, \"edge_probs_final.pt\")\n",
    "        torch.save(edge_probs, edge_probs_path)\n",
    "        args.edge_probs_file = edge_probs_path\n",
    "    \n",
    "    logs.write_to_log_file('Data: '+ suffix)\n",
    "    logs.write_to_log_file('Version: '+ version)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time-start_time\n",
    "    print(f'Total Time: {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n",
    "    logs.write_to_log_file(f'Total Time: {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- run MLR on energy data --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### train mlr baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlr_args = parse_args(\n",
    "    epochs=epochs,\n",
    "    training_samples=num_samples_train*num_timesteps, \n",
    "    test_samples=num_samples_test*num_timesteps,\n",
    "    shuffle_traindata=True,\n",
    "    suffix=suffix+'_MLR',\n",
    "    timesteps=1,\n",
    "    num_atoms=num_ts,\n",
    "    dims=num_dims,\n",
    "    no_cuda=no_cuda,\n",
    "    dont_use_encoder=dont_use_encoder,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting_time: 2022-12-13 20:40:54.312457\n",
      "<class 'str'>\n",
      "Namespace(seed=969491451, GPU_to_use=None, epochs=100, batch_size=16, lr=0.0005, lr_decay=200, gamma=0.5, training_samples=105216, test_samples=35040, shuffle_traindata=True, prediction_steps=10, encoder_hidden=256, decoder_hidden=256, encoder='mlp', decoder='mlp', prior=1, edge_types=2, dont_use_encoder=False, lr_z=0.1, global_temp=False, load_temperatures=False, alpha=2, num_cats=3, unobserved=0, model_unobserved=0, dont_shuffle_unobserved=False, teacher_forcing=0, suffix='_energy_long_1d_MLR', timesteps=1, num_atoms=7, dims=11, datadir='./data', save_folder='logs', expername='', sym_save_folder='../logs', load_folder='', test_time_adapt=False, lr_logits=0.01, num_tta_steps=100, dont_skip_first=False, temp=0.5, hard=False, no_validate=False, no_cuda=False, var=5e-07, encoder_dropout=0.0, decoder_dropout=0.0, no_factor=False, f='C:\\\\Users\\\\GwenH\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-4180d1bd-7365-43f6-9be5-09d102f5e672.json', test=True, device=device(type='cuda', index=0), cuda=True, factor=True, validate=True, shuffle_unobserved=True, skip_first=True, use_encoder=True, time='20221213-204054', num_GPU=1, batch_size_multiGPU=16, log_path='logs\\\\20221213-204054')\n",
      "i=10 target load_actual train_loss 1.3859845399856567 val_loss 0.6220945715904236\n",
      "i=20 target load_actual train_loss 0.619264543056488 val_loss 0.3168906569480896\n",
      "i=30 target load_actual train_loss 0.38256552815437317 val_loss 0.2224297672510147\n",
      "i=40 target load_actual train_loss 0.23020608723163605 val_loss 0.13398295640945435\n",
      "i=50 target load_actual train_loss 0.14387254416942596 val_loss 0.12955515086650848\n",
      "i=60 target load_actual train_loss 0.09763472527265549 val_loss 0.09256069362163544\n",
      "i=70 target load_actual train_loss 0.06542865931987762 val_loss 0.07160878926515579\n",
      "i=80 target load_actual train_loss 0.04669112712144852 val_loss 0.05547766014933586\n",
      "i=90 target load_actual train_loss 0.03452012315392494 val_loss 0.04581473767757416\n",
      "i=100 target load_actual train_loss 0.026732664555311203 val_loss 0.03952133283019066\n",
      "target = load_actual, test loss = 0.0273591335862875\n",
      "i=10 target solar_generation train_loss 3.3853678703308105 val_loss 5.25514554977417\n",
      "i=20 target solar_generation train_loss 1.3744815587997437 val_loss 0.5378851890563965\n",
      "i=30 target solar_generation train_loss 0.34833142161369324 val_loss 0.38410890102386475\n",
      "i=40 target solar_generation train_loss 0.05179113149642944 val_loss 0.02426442690193653\n",
      "i=50 target solar_generation train_loss 0.026145409792661667 val_loss 0.03271562233567238\n",
      "i=60 target solar_generation train_loss 0.022412290796637535 val_loss 0.009446114301681519\n",
      "i=70 target solar_generation train_loss 0.013869358226656914 val_loss 0.010296035557985306\n",
      "i=80 target solar_generation train_loss 0.007527564186602831 val_loss 0.0046122134663164616\n",
      "i=90 target solar_generation train_loss 0.005906751379370689 val_loss 0.0045372555032372475\n",
      "i=100 target solar_generation train_loss 0.005523898173123598 val_loss 0.002994453301653266\n",
      "target = solar_generation, test loss = 0.007145353127270937\n",
      "i=10 target wind_generation train_loss 1.0535986423492432 val_loss 0.9683976769447327\n",
      "i=20 target wind_generation train_loss 1.3462682962417603 val_loss 1.1022982597351074\n",
      "i=30 target wind_generation train_loss 0.3433730900287628 val_loss 0.5941668748855591\n",
      "i=40 target wind_generation train_loss 0.2278636246919632 val_loss 0.19102700054645538\n",
      "i=50 target wind_generation train_loss 0.14989154040813446 val_loss 0.18428009748458862\n",
      "i=60 target wind_generation train_loss 0.104823037981987 val_loss 0.22326456010341644\n",
      "i=70 target wind_generation train_loss 0.08807455003261566 val_loss 0.15491703152656555\n",
      "i=80 target wind_generation train_loss 0.08135920763015747 val_loss 0.1339668333530426\n",
      "i=90 target wind_generation train_loss 0.07256665825843811 val_loss 0.14515990018844604\n",
      "i=100 target wind_generation train_loss 0.06624723970890045 val_loss 0.12702715396881104\n",
      "target = wind_generation, test loss = 0.10341296344995499\n",
      "i=10 target price_day_ahead train_loss 0.7882751822471619 val_loss 0.2203768789768219\n",
      "i=20 target price_day_ahead train_loss 0.30793389678001404 val_loss 0.12234143912792206\n",
      "i=30 target price_day_ahead train_loss 0.1323145627975464 val_loss 0.08871781826019287\n",
      "i=40 target price_day_ahead train_loss 0.08157376199960709 val_loss 0.04869547113776207\n",
      "i=50 target price_day_ahead train_loss 0.05827896296977997 val_loss 0.042744215577840805\n",
      "i=60 target price_day_ahead train_loss 0.043446607887744904 val_loss 0.037505533546209335\n",
      "i=70 target price_day_ahead train_loss 0.03486610949039459 val_loss 0.029422927647829056\n",
      "i=80 target price_day_ahead train_loss 0.02923581935465336 val_loss 0.030097927898168564\n",
      "i=90 target price_day_ahead train_loss 0.025122538208961487 val_loss 0.024540578946471214\n",
      "i=100 target price_day_ahead train_loss 0.021989835426211357 val_loss 0.024405162781476974\n",
      "target = price_day_ahead, test loss = 0.03216046094894409\n",
      "i=10 target temperature train_loss 1.228305459022522 val_loss 3.5292444229125977\n",
      "i=20 target temperature train_loss 0.36339566111564636 val_loss 0.4747750163078308\n",
      "i=30 target temperature train_loss 0.0988961011171341 val_loss 0.3188714385032654\n",
      "i=40 target temperature train_loss 0.06111033633351326 val_loss 0.1603732407093048\n",
      "i=50 target temperature train_loss 0.013243394903838634 val_loss 0.009496196173131466\n",
      "i=60 target temperature train_loss 0.008991710841655731 val_loss 0.028291678056120872\n",
      "i=70 target temperature train_loss 0.0023090641479939222 val_loss 0.0025362649466842413\n",
      "i=80 target temperature train_loss 0.0014626733027398586 val_loss 0.0023879457730799913\n",
      "i=90 target temperature train_loss 0.000997247756458819 val_loss 0.0013793997932225466\n",
      "i=100 target temperature train_loss 0.0007727201445959508 val_loss 0.0008538061520084739\n",
      "target = temperature, test loss = 0.0008912762277759612\n",
      "i=10 target radiation_direct_horizontal train_loss 1.9508477449417114 val_loss 2.226421356201172\n",
      "i=20 target radiation_direct_horizontal train_loss 0.5171335935592651 val_loss 0.5902981162071228\n",
      "i=30 target radiation_direct_horizontal train_loss 0.3696918785572052 val_loss 0.2526901066303253\n",
      "i=40 target radiation_direct_horizontal train_loss 0.25746363401412964 val_loss 0.10909527540206909\n",
      "i=50 target radiation_direct_horizontal train_loss 0.17453277111053467 val_loss 0.09319689869880676\n",
      "i=60 target radiation_direct_horizontal train_loss 0.14510604739189148 val_loss 0.10041362047195435\n",
      "i=70 target radiation_direct_horizontal train_loss 0.11192892491817474 val_loss 0.06258942186832428\n",
      "i=80 target radiation_direct_horizontal train_loss 0.09319736808538437 val_loss 0.043207235634326935\n",
      "i=90 target radiation_direct_horizontal train_loss 0.07727877050638199 val_loss 0.04142744466662407\n",
      "i=100 target radiation_direct_horizontal train_loss 0.0653667002916336 val_loss 0.033145129680633545\n",
      "target = radiation_direct_horizontal, test loss = 0.08340201526880264\n",
      "i=10 target radiation_diffuse_horizontal train_loss 0.7325984239578247 val_loss 0.7194778323173523\n",
      "i=20 target radiation_diffuse_horizontal train_loss 0.4675339460372925 val_loss 0.26242387294769287\n",
      "i=30 target radiation_diffuse_horizontal train_loss 0.20576490461826324 val_loss 0.20495755970478058\n",
      "i=40 target radiation_diffuse_horizontal train_loss 0.11722511053085327 val_loss 0.1390419602394104\n",
      "i=50 target radiation_diffuse_horizontal train_loss 0.07142223417758942 val_loss 0.08876613527536392\n",
      "i=60 target radiation_diffuse_horizontal train_loss 0.04714995622634888 val_loss 0.04834360629320145\n",
      "i=70 target radiation_diffuse_horizontal train_loss 0.032233431935310364 val_loss 0.03667716681957245\n",
      "i=80 target radiation_diffuse_horizontal train_loss 0.023751236498355865 val_loss 0.022318601608276367\n",
      "i=90 target radiation_diffuse_horizontal train_loss 0.018259197473526 val_loss 0.016272785142064095\n",
      "i=100 target radiation_diffuse_horizontal train_loss 0.015004413202404976 val_loss 0.014149625785648823\n",
      "target = radiation_diffuse_horizontal, test loss = 0.015266183763742447\n",
      "average test loss= 0.038519626624682654\n",
      "Data: _energy_long_1d\n",
      "Version: gcn\n",
      "Total Time: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "if run_mlr:\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting_time: {datetime.datetime.fromtimestamp(start_time)}\")\n",
    "    \n",
    "    logs = Logger(mlr_args)\n",
    "\n",
    "    MLR_models, df_MLRpred_norm, test_losses, avg_test_loss = run_MLR(dataframe, \n",
    "                                  cols_ts, cols_timedims,\n",
    "                                  num_samples_train=mlr_args.training_samples,\n",
    "                                  num_samples_valid=num_samples_valid,\n",
    "                                  num_samples_test=mlr_args.test_samples,\n",
    "                                  epochs=epochs, print_every=10)\n",
    "    \n",
    "    np.save(os.path.join(mlr_args.log_path, \"MLR_models\"+suffix[:-3]+\".npy\"), MLR_models)\n",
    "    \n",
    "    # df_MLRpred = denormalize_columns(df_MLRpred_norm, suffix=suffix)\n",
    "    # if save_pred_csv:\n",
    "    #     df_MLRpred.to_csv('data/mlr_pred_'+ suffix + '.csv', index=False)\n",
    "        \n",
    "    logs.write_to_log_file('Data: '+ suffix)\n",
    "    logs.write_to_log_file('Version: '+ version)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time-start_time\n",
    "    logs.write_to_log_file(f'Total Time: {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>ID</th>\n",
       "      <th>load_actual</th>\n",
       "      <th>solar_generation</th>\n",
       "      <th>wind_generation</th>\n",
       "      <th>price_day_ahead</th>\n",
       "      <th>temperature</th>\n",
       "      <th>radiation_direct_horizontal</th>\n",
       "      <th>radiation_diffuse_horizontal</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>load_actual_pred</th>\n",
       "      <th>solar_generation_pred</th>\n",
       "      <th>wind_generation_pred</th>\n",
       "      <th>price_day_ahead_pred</th>\n",
       "      <th>temperature_pred</th>\n",
       "      <th>radiation_direct_horizontal_pred</th>\n",
       "      <th>radiation_diffuse_horizontal_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.868881</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999917</td>\n",
       "      <td>-0.036019</td>\n",
       "      <td>-0.779024</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.867209</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999286</td>\n",
       "      <td>-0.052502</td>\n",
       "      <td>-0.770590</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-1.161181</td>\n",
       "      <td>-1.097711</td>\n",
       "      <td>-1.124598</td>\n",
       "      <td>-0.212668</td>\n",
       "      <td>-0.786290</td>\n",
       "      <td>-1.410044</td>\n",
       "      <td>-0.916272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.872605</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999347</td>\n",
       "      <td>-0.110902</td>\n",
       "      <td>-0.759535</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-1.116041</td>\n",
       "      <td>-1.080096</td>\n",
       "      <td>-1.099129</td>\n",
       "      <td>-0.206513</td>\n",
       "      <td>-0.769359</td>\n",
       "      <td>-1.358665</td>\n",
       "      <td>-0.889505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.873883</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999300</td>\n",
       "      <td>-0.139392</td>\n",
       "      <td>-0.747129</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-1.041792</td>\n",
       "      <td>-1.061131</td>\n",
       "      <td>-1.008731</td>\n",
       "      <td>-0.207184</td>\n",
       "      <td>-0.751144</td>\n",
       "      <td>-1.299256</td>\n",
       "      <td>-0.858701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.881053</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.999484</td>\n",
       "      <td>-0.190042</td>\n",
       "      <td>-0.737589</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.953795</td>\n",
       "      <td>-1.040070</td>\n",
       "      <td>-0.934987</td>\n",
       "      <td>-0.184219</td>\n",
       "      <td>-0.730938</td>\n",
       "      <td>-1.229738</td>\n",
       "      <td>-0.829269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175291</th>\n",
       "      <td>2019-12-31 19:00:00+00:00</td>\n",
       "      <td>261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.167290</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.754123</td>\n",
       "      <td>-0.100095</td>\n",
       "      <td>-0.185866</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.083458</td>\n",
       "      <td>-1.070426</td>\n",
       "      <td>-0.493131</td>\n",
       "      <td>0.045321</td>\n",
       "      <td>-0.216586</td>\n",
       "      <td>-1.197622</td>\n",
       "      <td>-1.133252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175292</th>\n",
       "      <td>2019-12-31 20:00:00+00:00</td>\n",
       "      <td>261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.225331</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.742449</td>\n",
       "      <td>-0.114941</td>\n",
       "      <td>-0.191598</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.205058</td>\n",
       "      <td>-1.085295</td>\n",
       "      <td>-0.497325</td>\n",
       "      <td>-0.043705</td>\n",
       "      <td>-0.224200</td>\n",
       "      <td>-1.257187</td>\n",
       "      <td>-1.148364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175293</th>\n",
       "      <td>2019-12-31 21:00:00+00:00</td>\n",
       "      <td>261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.283632</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.738013</td>\n",
       "      <td>-0.153037</td>\n",
       "      <td>-0.201384</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.341897</td>\n",
       "      <td>-1.094167</td>\n",
       "      <td>-0.564585</td>\n",
       "      <td>-0.105669</td>\n",
       "      <td>-0.229758</td>\n",
       "      <td>-1.298907</td>\n",
       "      <td>-1.158824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175294</th>\n",
       "      <td>2019-12-31 22:00:00+00:00</td>\n",
       "      <td>261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.361136</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.716551</td>\n",
       "      <td>-0.169083</td>\n",
       "      <td>-0.213626</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.452277</td>\n",
       "      <td>-1.098482</td>\n",
       "      <td>-0.595025</td>\n",
       "      <td>-0.166660</td>\n",
       "      <td>-0.237636</td>\n",
       "      <td>-1.326830</td>\n",
       "      <td>-1.156580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175295</th>\n",
       "      <td>2019-12-31 23:00:00+00:00</td>\n",
       "      <td>261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.368557</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.722311</td>\n",
       "      <td>-0.090599</td>\n",
       "      <td>-0.226483</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.570629</td>\n",
       "      <td>-1.098007</td>\n",
       "      <td>-0.642688</td>\n",
       "      <td>-0.216359</td>\n",
       "      <td>-0.244574</td>\n",
       "      <td>-1.336440</td>\n",
       "      <td>-1.148326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175296 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   utc_timestamp  week  year  month  day  hour  weekday  \\\n",
       "0      2015-01-01 00:00:00+00:00     0  -1.0      0    0     0        3   \n",
       "1      2015-01-01 01:00:00+00:00     0  -1.0      0    0     1        3   \n",
       "2      2015-01-01 02:00:00+00:00     0  -1.0      0    0     2        3   \n",
       "3      2015-01-01 03:00:00+00:00     0  -1.0      0    0     3        3   \n",
       "4      2015-01-01 04:00:00+00:00     0  -1.0      0    0     4        3   \n",
       "...                          ...   ...   ...    ...  ...   ...      ...   \n",
       "175291 2019-12-31 19:00:00+00:00   261   3.0     11   30    19        1   \n",
       "175292 2019-12-31 20:00:00+00:00   261   3.0     11   30    20        1   \n",
       "175293 2019-12-31 21:00:00+00:00   261   3.0     11   30    21        1   \n",
       "175294 2019-12-31 22:00:00+00:00   261   3.0     11   30    22        1   \n",
       "175295 2019-12-31 23:00:00+00:00   261   3.0     11   30    23        1   \n",
       "\n",
       "              ID  load_actual  solar_generation  wind_generation  \\\n",
       "0       0.333333    -0.868881         -0.999995        -0.999917   \n",
       "1       0.333333    -0.867209         -0.999996        -0.999286   \n",
       "2       0.333333    -0.872605         -0.999995        -0.999347   \n",
       "3       0.333333    -0.873883         -0.999995        -0.999300   \n",
       "4       0.333333    -0.881053         -0.999995        -0.999484   \n",
       "...          ...          ...               ...              ...   \n",
       "175291  1.000000    -0.167290         -1.000000        -0.754123   \n",
       "175292  1.000000    -0.225331         -1.000000        -0.742449   \n",
       "175293  1.000000    -0.283632         -1.000000        -0.738013   \n",
       "175294  1.000000    -0.361136         -1.000000        -0.716551   \n",
       "175295  1.000000    -0.368557         -1.000000        -0.722311   \n",
       "\n",
       "        price_day_ahead  temperature  radiation_direct_horizontal  \\\n",
       "0             -0.036019    -0.779024                         -1.0   \n",
       "1             -0.052502    -0.770590                         -1.0   \n",
       "2             -0.110902    -0.759535                         -1.0   \n",
       "3             -0.139392    -0.747129                         -1.0   \n",
       "4             -0.190042    -0.737589                         -1.0   \n",
       "...                 ...          ...                          ...   \n",
       "175291        -0.100095    -0.185866                         -1.0   \n",
       "175292        -0.114941    -0.191598                         -1.0   \n",
       "175293        -0.153037    -0.201384                         -1.0   \n",
       "175294        -0.169083    -0.213626                         -1.0   \n",
       "175295        -0.090599    -0.226483                         -1.0   \n",
       "\n",
       "        radiation_diffuse_horizontal  month_sin  month_cos   day_sin  day_cos  \\\n",
       "0                               -1.0        0.0   1.000000  0.000000  1.00000   \n",
       "1                               -1.0        0.0   1.000000  0.000000  1.00000   \n",
       "2                               -1.0        0.0   1.000000  0.000000  1.00000   \n",
       "3                               -1.0        0.0   1.000000  0.000000  1.00000   \n",
       "4                               -1.0        0.0   1.000000  0.000000  1.00000   \n",
       "...                              ...        ...        ...       ...      ...   \n",
       "175291                          -1.0       -0.5   0.866025 -0.201299  0.97953   \n",
       "175292                          -1.0       -0.5   0.866025 -0.201299  0.97953   \n",
       "175293                          -1.0       -0.5   0.866025 -0.201299  0.97953   \n",
       "175294                          -1.0       -0.5   0.866025 -0.201299  0.97953   \n",
       "175295                          -1.0       -0.5   0.866025 -0.201299  0.97953   \n",
       "\n",
       "        hour_sin  hour_cos  weekday_sin  weekday_cos  load_actual_pred  \\\n",
       "0       0.000000  1.000000     0.433884    -0.900969          0.000000   \n",
       "1       0.258819  0.965926     0.433884    -0.900969         -1.161181   \n",
       "2       0.500000  0.866025     0.433884    -0.900969         -1.116041   \n",
       "3       0.707107  0.707107     0.433884    -0.900969         -1.041792   \n",
       "4       0.866025  0.500000     0.433884    -0.900969         -0.953795   \n",
       "...          ...       ...          ...          ...               ...   \n",
       "175291 -0.965926  0.258819     0.781831     0.623490         -0.083458   \n",
       "175292 -0.866025  0.500000     0.781831     0.623490         -0.205058   \n",
       "175293 -0.707107  0.707107     0.781831     0.623490         -0.341897   \n",
       "175294 -0.500000  0.866025     0.781831     0.623490         -0.452277   \n",
       "175295 -0.258819  0.965926     0.781831     0.623490         -0.570629   \n",
       "\n",
       "        solar_generation_pred  wind_generation_pred  price_day_ahead_pred  \\\n",
       "0                    0.000000              0.000000              0.000000   \n",
       "1                   -1.097711             -1.124598             -0.212668   \n",
       "2                   -1.080096             -1.099129             -0.206513   \n",
       "3                   -1.061131             -1.008731             -0.207184   \n",
       "4                   -1.040070             -0.934987             -0.184219   \n",
       "...                       ...                   ...                   ...   \n",
       "175291              -1.070426             -0.493131              0.045321   \n",
       "175292              -1.085295             -0.497325             -0.043705   \n",
       "175293              -1.094167             -0.564585             -0.105669   \n",
       "175294              -1.098482             -0.595025             -0.166660   \n",
       "175295              -1.098007             -0.642688             -0.216359   \n",
       "\n",
       "        temperature_pred  radiation_direct_horizontal_pred  \\\n",
       "0               0.000000                          0.000000   \n",
       "1              -0.786290                         -1.410044   \n",
       "2              -0.769359                         -1.358665   \n",
       "3              -0.751144                         -1.299256   \n",
       "4              -0.730938                         -1.229738   \n",
       "...                  ...                               ...   \n",
       "175291         -0.216586                         -1.197622   \n",
       "175292         -0.224200                         -1.257187   \n",
       "175293         -0.229758                         -1.298907   \n",
       "175294         -0.237636                         -1.326830   \n",
       "175295         -0.244574                         -1.336440   \n",
       "\n",
       "        radiation_diffuse_horizontal_pred  \n",
       "0                                0.000000  \n",
       "1                               -0.916272  \n",
       "2                               -0.889505  \n",
       "3                               -0.858701  \n",
       "4                               -0.829269  \n",
       "...                                   ...  \n",
       "175291                          -1.133252  \n",
       "175292                          -1.148364  \n",
       "175293                          -1.158824  \n",
       "175294                          -1.156580  \n",
       "175295                          -1.148326  \n",
       "\n",
       "[175296 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### -------------------- run RNN on energy data --------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### train rnn baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_samples=num_samples_train\n",
    "test_samples=num_samples_test\n",
    "timesteps=num_timesteps\n",
    "num_atoms=num_ts\n",
    "dims=num_dims\n",
    "lstm_layers = 1\n",
    "input_dim = num_atoms+len(cols_timedims)\n",
    "target_dim = num_atoms\n",
    "hidden_dim = 128\n",
    "\n",
    "rnn_args = parse_args(\n",
    "    epochs=epochs, \n",
    "    training_samples=training_samples, \n",
    "    test_samples=test_samples,\n",
    "    shuffle_traindata=True,\n",
    "    suffix=suffix,\n",
    "    timesteps=timesteps,\n",
    "    num_atoms=num_atoms,\n",
    "    dims=dims,\n",
    "    no_cuda=no_cuda,\n",
    "    dont_use_encoder=dont_use_encoder,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if run_rnn:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Starting_time: {datetime.datetime.fromtimestamp(start_time)}\")\n",
    "    \n",
    "    logs = Logger(rnn_args)\n",
    "\n",
    "    if rnn_args.GPU_to_use is not None:\n",
    "        logs.write_to_log_file(\"Using GPU #\" + str(rnn_args.GPU_to_use))\n",
    "\n",
    "    (\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        test_loader,\n",
    "        loc_max,\n",
    "        loc_min,\n",
    "        vel_max,\n",
    "        vel_min,\n",
    "    ) = load_data(rnn_args)\n",
    "    \n",
    "    rnn = RNN_baseline(lstm_layers, \n",
    "                       input_dim, \n",
    "                       target_dim, \n",
    "                       batch_size, \n",
    "                       hidden_dim,\n",
    "                      rnn_args.device)\n",
    "    if rnn_args.cuda:\n",
    "        rnn = rnn.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(list(rnn.parameters()), lr=rnn_args.lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=rnn_args.lr_decay, \n",
    "                                    gamma=rnn_args.gamma)\n",
    "    \n",
    "    logs.write_to_log_file(rnn)\n",
    "\n",
    "    ##Train model\n",
    "    try:\n",
    "        if rnn_args.test_time_adapt:\n",
    "            raise KeyboardInterrupt\n",
    "\n",
    "        best_epoch, epoch = train_rnn(rnn_args)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        best_epoch, epoch = -1, -1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    logs.write_to_log_file(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "\n",
    "    if rnn_args.test:\n",
    "        test_rnn(rnn, epoch, rnn_args)\n",
    "    \n",
    "    logs.write_to_log_file('Data: '+ suffix)\n",
    "    logs.write_to_log_file('Version: '+ version)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time-start_time\n",
    "    print(f'Total Time: {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n",
    "    logs.write_to_log_file(f'Total Time: {time.strftime(\"%H:%M:%S\", time.gmtime(total_time))}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
